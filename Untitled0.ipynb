{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emmymarysia/Flixster/blob/main/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnF-H8ntqx86",
        "outputId": "feef3081-6172-4fce-dde1-8879baed1069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yPpbZsDBLfF",
        "outputId": "901a5982-9631-4d02-c2a9-b66ecf95d2d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.3.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epGzn5gODP_y",
        "outputId": "47597f8c-3431-4c86-c674-c2c710209ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.35.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.4.1)\n",
            "Collecting multiprocess (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (0.15.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, accelerate, datasets\n",
            "Successfully installed accelerate-0.25.0 datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxdrH6GxDWow",
        "outputId": "fbbbc946-3543-4484-e0f8-52c67d4871e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-12-01 22:46:28.513598: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-01 22:46:28.513667: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-01 22:46:28.513717: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-01 22:46:29.716881: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 3.82k/3.82k [00:00<00:00, 23.3MB/s]\n",
            "Downloading metadata: 100% 1.90k/1.90k [00:00<00:00, 14.6MB/s]\n",
            "Downloading readme: 100% 14.1k/14.1k [00:00<00:00, 39.4MB/s]\n",
            "Downloading: 100% 1.93k/1.93k [00:00<00:00, 7.93MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 9.12MB/s]\n",
            "Downloading: 100% 65.9M/65.9M [00:01<00:00, 59.8MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 9.12MB/s]\n",
            "config.json: 100% 665/665 [00:00<00:00, 3.04MB/s]\n",
            "pytorch_model.bin: 100% 54.2M/54.2M [00:00<00:00, 165MB/s]\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "tokenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 166kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 3.97MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 26.2MB/s]\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 108094.77 examples/s]\n",
            "Filter: 100% 550152/550152 [00:02<00:00, 260759.48 examples/s]\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 258486.42 examples/s]\n",
            "Map (num_proc=2): 100% 549367/549367 [01:38<00:00, 5579.30 examples/s]\n",
            "  0% 0/206013 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.8887, 'learning_rate': 4.987864843480751e-05, 'epoch': 0.01}\n",
            "{'loss': 0.7229, 'learning_rate': 4.975729686961503e-05, 'epoch': 0.01}\n",
            "{'loss': 0.6957, 'learning_rate': 4.963594530442254e-05, 'epoch': 0.02}\n",
            "{'loss': 0.6504, 'learning_rate': 4.951459373923005e-05, 'epoch': 0.03}\n",
            "{'loss': 0.6532, 'learning_rate': 4.9393242174037564e-05, 'epoch': 0.04}\n",
            "{'loss': 0.609, 'learning_rate': 4.927189060884507e-05, 'epoch': 0.04}\n",
            "{'loss': 0.6105, 'learning_rate': 4.9150539043652585e-05, 'epoch': 0.05}\n",
            "{'loss': 0.5995, 'learning_rate': 4.90291874784601e-05, 'epoch': 0.06}\n",
            "{'loss': 0.5868, 'learning_rate': 4.890783591326761e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5731, 'learning_rate': 4.8786484348075126e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5723, 'learning_rate': 4.866513278288264e-05, 'epoch': 0.08}\n",
            "{'loss': 0.5712, 'learning_rate': 4.8543781217690146e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5613, 'learning_rate': 4.842242965249766e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5456, 'learning_rate': 4.8301078087305174e-05, 'epoch': 0.1}\n",
            "{'loss': 0.5707, 'learning_rate': 4.817972652211268e-05, 'epoch': 0.11}\n",
            "{'loss': 0.5616, 'learning_rate': 4.80583749569202e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5302, 'learning_rate': 4.793702339172771e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5368, 'learning_rate': 4.781567182653522e-05, 'epoch': 0.13}\n",
            "{'loss': 0.5331, 'learning_rate': 4.7694320261342736e-05, 'epoch': 0.14}\n",
            "{'loss': 0.5351, 'learning_rate': 4.757296869615024e-05, 'epoch': 0.15}\n",
            "{'loss': 0.5309, 'learning_rate': 4.7451617130957756e-05, 'epoch': 0.15}\n",
            "{'loss': 0.5268, 'learning_rate': 4.733026556576527e-05, 'epoch': 0.16}\n",
            "{'loss': 0.5456, 'learning_rate': 4.720891400057278e-05, 'epoch': 0.17}\n",
            "{'loss': 0.5137, 'learning_rate': 4.70875624353803e-05, 'epoch': 0.17}\n",
            "{'loss': 0.534, 'learning_rate': 4.6966210870187804e-05, 'epoch': 0.18}\n",
            "{'loss': 0.5229, 'learning_rate': 4.684485930499532e-05, 'epoch': 0.19}\n",
            "{'loss': 0.5173, 'learning_rate': 4.672350773980283e-05, 'epoch': 0.2}\n",
            "{'loss': 0.5033, 'learning_rate': 4.660215617461034e-05, 'epoch': 0.2}\n",
            "{'loss': 0.5275, 'learning_rate': 4.648080460941785e-05, 'epoch': 0.21}\n",
            "{'loss': 0.5055, 'learning_rate': 4.6359453044225366e-05, 'epoch': 0.22}\n",
            "{'loss': 0.5075, 'learning_rate': 4.623810147903288e-05, 'epoch': 0.23}\n",
            "{'loss': 0.5094, 'learning_rate': 4.611674991384039e-05, 'epoch': 0.23}\n",
            "{'loss': 0.503, 'learning_rate': 4.599539834864791e-05, 'epoch': 0.24}\n",
            "{'loss': 0.4842, 'learning_rate': 4.5874046783455414e-05, 'epoch': 0.25}\n",
            "{'loss': 0.5123, 'learning_rate': 4.575269521826293e-05, 'epoch': 0.25}\n",
            "{'loss': 0.4829, 'learning_rate': 4.563134365307044e-05, 'epoch': 0.26}\n",
            "{'loss': 0.4988, 'learning_rate': 4.550999208787795e-05, 'epoch': 0.27}\n",
            "{'loss': 0.483, 'learning_rate': 4.538864052268547e-05, 'epoch': 0.28}\n",
            "{'loss': 0.5026, 'learning_rate': 4.5267288957492976e-05, 'epoch': 0.28}\n",
            "{'loss': 0.5187, 'learning_rate': 4.514593739230048e-05, 'epoch': 0.29}\n",
            "{'loss': 0.5022, 'learning_rate': 4.5024585827108e-05, 'epoch': 0.3}\n",
            "{'loss': 0.5125, 'learning_rate': 4.490323426191551e-05, 'epoch': 0.31}\n",
            "{'loss': 0.5035, 'learning_rate': 4.4781882696723024e-05, 'epoch': 0.31}\n",
            "{'loss': 0.4846, 'learning_rate': 4.466053113153054e-05, 'epoch': 0.32}\n",
            "{'loss': 0.4889, 'learning_rate': 4.4539179566338044e-05, 'epoch': 0.33}\n",
            "{'loss': 0.4826, 'learning_rate': 4.4417828001145565e-05, 'epoch': 0.33}\n",
            "{'loss': 0.483, 'learning_rate': 4.429647643595307e-05, 'epoch': 0.34}\n",
            "{'loss': 0.4818, 'learning_rate': 4.4175124870760585e-05, 'epoch': 0.35}\n",
            "{'loss': 0.5081, 'learning_rate': 4.40537733055681e-05, 'epoch': 0.36}\n",
            "{'loss': 0.4728, 'learning_rate': 4.393242174037561e-05, 'epoch': 0.36}\n",
            "{'loss': 0.457, 'learning_rate': 4.381107017518312e-05, 'epoch': 0.37}\n",
            "{'loss': 0.4742, 'learning_rate': 4.368971860999063e-05, 'epoch': 0.38}\n",
            "{'loss': 0.4955, 'learning_rate': 4.356836704479815e-05, 'epoch': 0.39}\n",
            "{'loss': 0.4888, 'learning_rate': 4.3447015479605654e-05, 'epoch': 0.39}\n",
            "{'loss': 0.4774, 'learning_rate': 4.3325663914413174e-05, 'epoch': 0.4}\n",
            "{'loss': 0.4875, 'learning_rate': 4.320431234922068e-05, 'epoch': 0.41}\n",
            "{'loss': 0.4811, 'learning_rate': 4.3082960784028195e-05, 'epoch': 0.42}\n",
            "{'loss': 0.4869, 'learning_rate': 4.296160921883571e-05, 'epoch': 0.42}\n",
            "{'loss': 0.4833, 'learning_rate': 4.2840257653643216e-05, 'epoch': 0.43}\n",
            "{'loss': 0.4729, 'learning_rate': 4.2718906088450736e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4813, 'learning_rate': 4.259755452325824e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4597, 'learning_rate': 4.247620295806575e-05, 'epoch': 0.45}\n",
            "{'loss': 0.4757, 'learning_rate': 4.235485139287327e-05, 'epoch': 0.46}\n",
            "{'loss': 0.4931, 'learning_rate': 4.223349982768078e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4842, 'learning_rate': 4.211214826248829e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4696, 'learning_rate': 4.1990796697295805e-05, 'epoch': 0.48}\n",
            "{'loss': 0.4591, 'learning_rate': 4.186944513210331e-05, 'epoch': 0.49}\n",
            "{'loss': 0.4606, 'learning_rate': 4.1748093566910825e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4627, 'learning_rate': 4.162674200171834e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4706, 'learning_rate': 4.150539043652585e-05, 'epoch': 0.51}\n",
            "{'loss': 0.4934, 'learning_rate': 4.1384038871333367e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4527, 'learning_rate': 4.126268730614088e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4881, 'learning_rate': 4.114133574094839e-05, 'epoch': 0.53}\n",
            "{'loss': 0.4774, 'learning_rate': 4.10199841757559e-05, 'epoch': 0.54}\n",
            "{'loss': 0.4577, 'learning_rate': 4.0898632610563415e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4704, 'learning_rate': 4.077728104537092e-05, 'epoch': 0.55}\n",
            "{'loss': 0.473, 'learning_rate': 4.065592948017844e-05, 'epoch': 0.56}\n",
            "{'loss': 0.49, 'learning_rate': 4.053457791498595e-05, 'epoch': 0.57}\n",
            "{'loss': 0.4461, 'learning_rate': 4.041322634979346e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4582, 'learning_rate': 4.0291874784600976e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4526, 'learning_rate': 4.017052321940848e-05, 'epoch': 0.59}\n",
            "{'loss': 0.4671, 'learning_rate': 4.0049171654216e-05, 'epoch': 0.6}\n",
            "{'loss': 0.4554, 'learning_rate': 3.992782008902351e-05, 'epoch': 0.6}\n",
            "{'loss': 0.473, 'learning_rate': 3.980646852383102e-05, 'epoch': 0.61}\n",
            "{'loss': 0.4606, 'learning_rate': 3.968511695863854e-05, 'epoch': 0.62}\n",
            "{'loss': 0.4663, 'learning_rate': 3.9563765393446045e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4632, 'learning_rate': 3.944241382825356e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4532, 'learning_rate': 3.932106226306107e-05, 'epoch': 0.64}\n",
            "{'loss': 0.4762, 'learning_rate': 3.9199710697868586e-05, 'epoch': 0.65}\n",
            "{'loss': 0.4545, 'learning_rate': 3.907835913267609e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4575, 'learning_rate': 3.8957007567483607e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4574, 'learning_rate': 3.883565600229112e-05, 'epoch': 0.67}\n",
            "{'loss': 0.4531, 'learning_rate': 3.8714304437098634e-05, 'epoch': 0.68}\n",
            "{'loss': 0.4662, 'learning_rate': 3.859295287190615e-05, 'epoch': 0.68}\n",
            "{'loss': 0.4575, 'learning_rate': 3.8471601306713655e-05, 'epoch': 0.69}\n",
            "{'loss': 0.466, 'learning_rate': 3.835024974152117e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4554, 'learning_rate': 3.822889817632868e-05, 'epoch': 0.71}\n",
            "{'loss': 0.4426, 'learning_rate': 3.810754661113619e-05, 'epoch': 0.71}\n",
            "{'loss': 0.4646, 'learning_rate': 3.798619504594371e-05, 'epoch': 0.72}\n",
            "{'loss': 0.4601, 'learning_rate': 3.7864843480751216e-05, 'epoch': 0.73}\n",
            "{'loss': 0.4648, 'learning_rate': 3.774349191555872e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4592, 'learning_rate': 3.7622140350366244e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4225, 'learning_rate': 3.750078878517375e-05, 'epoch': 0.75}\n",
            "{'loss': 0.4573, 'learning_rate': 3.7379437219981264e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4507, 'learning_rate': 3.725808565478878e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4531, 'learning_rate': 3.7136734089596285e-05, 'epoch': 0.77}\n",
            "{'loss': 0.4474, 'learning_rate': 3.7015382524403805e-05, 'epoch': 0.78}\n",
            "{'loss': 0.4583, 'learning_rate': 3.689403095921131e-05, 'epoch': 0.79}\n",
            "{'loss': 0.4335, 'learning_rate': 3.6772679394018826e-05, 'epoch': 0.79}\n",
            "{'loss': 0.4467, 'learning_rate': 3.665132782882634e-05, 'epoch': 0.8}\n",
            "{'loss': 0.4505, 'learning_rate': 3.6529976263633853e-05, 'epoch': 0.81}\n",
            "{'loss': 0.4348, 'learning_rate': 3.640862469844136e-05, 'epoch': 0.82}\n",
            "{'loss': 0.4587, 'learning_rate': 3.6287273133248874e-05, 'epoch': 0.82}\n",
            "{'loss': 0.4357, 'learning_rate': 3.616592156805639e-05, 'epoch': 0.83}\n",
            "{'loss': 0.4391, 'learning_rate': 3.6044570002863895e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4278, 'learning_rate': 3.5923218437671415e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4493, 'learning_rate': 3.580186687247892e-05, 'epoch': 0.85}\n",
            "{'loss': 0.4749, 'learning_rate': 3.5680515307286436e-05, 'epoch': 0.86}\n",
            "{'loss': 0.4424, 'learning_rate': 3.555916374209395e-05, 'epoch': 0.87}\n",
            "{'loss': 0.4412, 'learning_rate': 3.5437812176901456e-05, 'epoch': 0.87}\n",
            "{'loss': 0.4397, 'learning_rate': 3.531646061170898e-05, 'epoch': 0.88}\n",
            "{'loss': 0.4352, 'learning_rate': 3.5195109046516484e-05, 'epoch': 0.89}\n",
            "{'loss': 0.4478, 'learning_rate': 3.507375748132399e-05, 'epoch': 0.9}\n",
            "{'loss': 0.4473, 'learning_rate': 3.495240591613151e-05, 'epoch': 0.9}\n",
            "{'loss': 0.4522, 'learning_rate': 3.483105435093902e-05, 'epoch': 0.91}\n",
            "{'loss': 0.4321, 'learning_rate': 3.470970278574653e-05, 'epoch': 0.92}\n",
            "{'loss': 0.4377, 'learning_rate': 3.4588351220554045e-05, 'epoch': 0.92}\n",
            "{'loss': 0.4351, 'learning_rate': 3.446699965536155e-05, 'epoch': 0.93}\n",
            "{'loss': 0.4407, 'learning_rate': 3.4345648090169066e-05, 'epoch': 0.94}\n",
            "{'loss': 0.4445, 'learning_rate': 3.422429652497658e-05, 'epoch': 0.95}\n",
            "{'loss': 0.4408, 'learning_rate': 3.4102944959784094e-05, 'epoch': 0.95}\n",
            "{'loss': 0.4524, 'learning_rate': 3.398159339459161e-05, 'epoch': 0.96}\n",
            "{'loss': 0.4502, 'learning_rate': 3.386024182939912e-05, 'epoch': 0.97}\n",
            "{'loss': 0.4131, 'learning_rate': 3.373889026420663e-05, 'epoch': 0.98}\n",
            "{'loss': 0.4251, 'learning_rate': 3.361753869901414e-05, 'epoch': 0.98}\n",
            "{'loss': 0.4205, 'learning_rate': 3.3496187133821655e-05, 'epoch': 0.99}\n",
            "{'loss': 0.4396, 'learning_rate': 3.337483556862916e-05, 'epoch': 1.0}\n",
            "{'loss': 0.4085, 'learning_rate': 3.325348400343668e-05, 'epoch': 1.0}\n",
            "{'loss': 0.4053, 'learning_rate': 3.313213243824419e-05, 'epoch': 1.01}\n",
            "{'loss': 0.4014, 'learning_rate': 3.30107808730517e-05, 'epoch': 1.02}\n",
            "{'loss': 0.4038, 'learning_rate': 3.288942930785922e-05, 'epoch': 1.03}\n",
            "{'loss': 0.3956, 'learning_rate': 3.2768077742666724e-05, 'epoch': 1.03}\n",
            "{'loss': 0.4207, 'learning_rate': 3.264672617747424e-05, 'epoch': 1.04}\n",
            "{'loss': 0.4079, 'learning_rate': 3.252537461228175e-05, 'epoch': 1.05}\n",
            "{'loss': 0.3907, 'learning_rate': 3.240402304708926e-05, 'epoch': 1.06}\n",
            "{'loss': 0.4276, 'learning_rate': 3.228267148189678e-05, 'epoch': 1.06}\n",
            "{'loss': 0.4099, 'learning_rate': 3.2161319916704286e-05, 'epoch': 1.07}\n",
            "{'loss': 0.4017, 'learning_rate': 3.20399683515118e-05, 'epoch': 1.08}\n",
            "{'loss': 0.4198, 'learning_rate': 3.191861678631931e-05, 'epoch': 1.08}\n",
            "{'loss': 0.4079, 'learning_rate': 3.179726522112683e-05, 'epoch': 1.09}\n",
            "{'loss': 0.4063, 'learning_rate': 3.1675913655934334e-05, 'epoch': 1.1}\n",
            "{'loss': 0.4111, 'learning_rate': 3.155456209074185e-05, 'epoch': 1.11}\n",
            "{'loss': 0.388, 'learning_rate': 3.143321052554936e-05, 'epoch': 1.11}\n",
            "{'loss': 0.4125, 'learning_rate': 3.131185896035687e-05, 'epoch': 1.12}\n",
            "{'loss': 0.394, 'learning_rate': 3.119050739516439e-05, 'epoch': 1.13}\n",
            "{'loss': 0.4048, 'learning_rate': 3.1069155829971895e-05, 'epoch': 1.14}\n",
            "{'loss': 0.4229, 'learning_rate': 3.094780426477941e-05, 'epoch': 1.14}\n",
            "{'loss': 0.3923, 'learning_rate': 3.082645269958692e-05, 'epoch': 1.15}\n",
            "{'loss': 0.4269, 'learning_rate': 3.070510113439443e-05, 'epoch': 1.16}\n",
            "{'loss': 0.4233, 'learning_rate': 3.058374956920195e-05, 'epoch': 1.16}\n",
            "{'loss': 0.4063, 'learning_rate': 3.0462398004009457e-05, 'epoch': 1.17}\n",
            "{'loss': 0.4119, 'learning_rate': 3.0341046438816967e-05, 'epoch': 1.18}\n",
            "{'loss': 0.4151, 'learning_rate': 3.021969487362448e-05, 'epoch': 1.19}\n",
            "{'loss': 0.4028, 'learning_rate': 3.009834330843199e-05, 'epoch': 1.19}\n",
            "{'loss': 0.4017, 'learning_rate': 2.997699174323951e-05, 'epoch': 1.2}\n",
            "{'loss': 0.3928, 'learning_rate': 2.985564017804702e-05, 'epoch': 1.21}\n",
            "{'loss': 0.4065, 'learning_rate': 2.9734288612854526e-05, 'epoch': 1.22}\n",
            "{'loss': 0.4184, 'learning_rate': 2.9612937047662043e-05, 'epoch': 1.22}\n",
            "{'loss': 0.3807, 'learning_rate': 2.9491585482469553e-05, 'epoch': 1.23}\n",
            "{'loss': 0.4056, 'learning_rate': 2.9370233917277067e-05, 'epoch': 1.24}\n",
            "{'loss': 0.4158, 'learning_rate': 2.9248882352084577e-05, 'epoch': 1.25}\n",
            "{'loss': 0.3866, 'learning_rate': 2.9127530786892094e-05, 'epoch': 1.25}\n",
            "{'loss': 0.4095, 'learning_rate': 2.9006179221699604e-05, 'epoch': 1.26}\n",
            "{'loss': 0.4035, 'learning_rate': 2.888482765650711e-05, 'epoch': 1.27}\n",
            "{'loss': 0.3947, 'learning_rate': 2.876347609131463e-05, 'epoch': 1.27}\n",
            "{'loss': 0.3871, 'learning_rate': 2.864212452612214e-05, 'epoch': 1.28}\n",
            "{'loss': 0.404, 'learning_rate': 2.8520772960929652e-05, 'epoch': 1.29}\n",
            "{'loss': 0.3975, 'learning_rate': 2.8399421395737163e-05, 'epoch': 1.3}\n",
            "{'loss': 0.4102, 'learning_rate': 2.8278069830544673e-05, 'epoch': 1.3}\n",
            "{'loss': 0.4296, 'learning_rate': 2.815671826535219e-05, 'epoch': 1.31}\n",
            "{'loss': 0.4046, 'learning_rate': 2.8035366700159697e-05, 'epoch': 1.32}\n",
            "{'loss': 0.4041, 'learning_rate': 2.7914015134967214e-05, 'epoch': 1.33}\n",
            "{'loss': 0.3945, 'learning_rate': 2.7792663569774724e-05, 'epoch': 1.33}\n",
            "{'loss': 0.4052, 'learning_rate': 2.7671312004582235e-05, 'epoch': 1.34}\n",
            "{'loss': 0.4076, 'learning_rate': 2.754996043938975e-05, 'epoch': 1.35}\n",
            "{'loss': 0.3946, 'learning_rate': 2.742860887419726e-05, 'epoch': 1.35}\n",
            "{'loss': 0.4267, 'learning_rate': 2.7307257309004776e-05, 'epoch': 1.36}\n",
            "{'loss': 0.4085, 'learning_rate': 2.7185905743812283e-05, 'epoch': 1.37}\n",
            "{'loss': 0.4116, 'learning_rate': 2.70645541786198e-05, 'epoch': 1.38}\n",
            "{'loss': 0.386, 'learning_rate': 2.694320261342731e-05, 'epoch': 1.38}\n",
            "{'loss': 0.4088, 'learning_rate': 2.682185104823482e-05, 'epoch': 1.39}\n",
            "{'loss': 0.3983, 'learning_rate': 2.6700499483042334e-05, 'epoch': 1.4}\n",
            "{'loss': 0.4059, 'learning_rate': 2.6579147917849845e-05, 'epoch': 1.41}\n",
            "{'loss': 0.3945, 'learning_rate': 2.645779635265736e-05, 'epoch': 1.41}\n",
            "{'loss': 0.3897, 'learning_rate': 2.633644478746487e-05, 'epoch': 1.42}\n",
            "{'loss': 0.4179, 'learning_rate': 2.621509322227238e-05, 'epoch': 1.43}\n",
            "{'loss': 0.4076, 'learning_rate': 2.6093741657079896e-05, 'epoch': 1.43}\n",
            "{'loss': 0.4151, 'learning_rate': 2.5972390091887406e-05, 'epoch': 1.44}\n",
            "{'loss': 0.391, 'learning_rate': 2.585103852669492e-05, 'epoch': 1.45}\n",
            "{'loss': 0.3905, 'learning_rate': 2.572968696150243e-05, 'epoch': 1.46}\n",
            "{'loss': 0.4043, 'learning_rate': 2.560833539630994e-05, 'epoch': 1.46}\n",
            "{'loss': 0.3861, 'learning_rate': 2.5486983831117454e-05, 'epoch': 1.47}\n",
            "{'loss': 0.3867, 'learning_rate': 2.5365632265924965e-05, 'epoch': 1.48}\n",
            "{'loss': 0.3992, 'learning_rate': 2.524428070073248e-05, 'epoch': 1.49}\n",
            "{'loss': 0.4038, 'learning_rate': 2.5122929135539992e-05, 'epoch': 1.49}\n",
            "{'loss': 0.4228, 'learning_rate': 2.5001577570347502e-05, 'epoch': 1.5}\n",
            "{'loss': 0.3915, 'learning_rate': 2.4880226005155016e-05, 'epoch': 1.51}\n",
            "{'loss': 0.3951, 'learning_rate': 2.475887443996253e-05, 'epoch': 1.51}\n",
            "{'loss': 0.3933, 'learning_rate': 2.463752287477004e-05, 'epoch': 1.52}\n",
            "{'loss': 0.4014, 'learning_rate': 2.451617130957755e-05, 'epoch': 1.53}\n",
            "{'loss': 0.3896, 'learning_rate': 2.4394819744385064e-05, 'epoch': 1.54}\n",
            "{'loss': 0.3806, 'learning_rate': 2.4273468179192578e-05, 'epoch': 1.54}\n",
            "{'loss': 0.4128, 'learning_rate': 2.4152116614000088e-05, 'epoch': 1.55}\n",
            "{'loss': 0.3715, 'learning_rate': 2.40307650488076e-05, 'epoch': 1.56}\n",
            "{'loss': 0.406, 'learning_rate': 2.3909413483615112e-05, 'epoch': 1.57}\n",
            "{'loss': 0.4056, 'learning_rate': 2.3788061918422626e-05, 'epoch': 1.57}\n",
            "{'loss': 0.4046, 'learning_rate': 2.3666710353230136e-05, 'epoch': 1.58}\n",
            "{'loss': 0.3797, 'learning_rate': 2.354535878803765e-05, 'epoch': 1.59}\n",
            "{'loss': 0.4194, 'learning_rate': 2.3424007222845163e-05, 'epoch': 1.59}\n",
            "{'loss': 0.3807, 'learning_rate': 2.3302655657652674e-05, 'epoch': 1.6}\n",
            "{'loss': 0.4084, 'learning_rate': 2.3181304092460184e-05, 'epoch': 1.61}\n",
            "{'loss': 0.4083, 'learning_rate': 2.3059952527267698e-05, 'epoch': 1.62}\n",
            "{'loss': 0.4087, 'learning_rate': 2.293860096207521e-05, 'epoch': 1.62}\n",
            "{'loss': 0.3917, 'learning_rate': 2.2817249396882722e-05, 'epoch': 1.63}\n",
            "{'loss': 0.3785, 'learning_rate': 2.2695897831690235e-05, 'epoch': 1.64}\n",
            "{'loss': 0.4033, 'learning_rate': 2.2574546266497746e-05, 'epoch': 1.65}\n",
            "{'loss': 0.4093, 'learning_rate': 2.245319470130526e-05, 'epoch': 1.65}\n",
            "{'loss': 0.4107, 'learning_rate': 2.233184313611277e-05, 'epoch': 1.66}\n",
            "{'loss': 0.3903, 'learning_rate': 2.2210491570920283e-05, 'epoch': 1.67}\n",
            "{'loss': 0.3883, 'learning_rate': 2.2089140005727797e-05, 'epoch': 1.67}\n",
            "{'loss': 0.4216, 'learning_rate': 2.1967788440535304e-05, 'epoch': 1.68}\n",
            "{'loss': 0.4111, 'learning_rate': 2.1846436875342818e-05, 'epoch': 1.69}\n",
            "{'loss': 0.3735, 'learning_rate': 2.172508531015033e-05, 'epoch': 1.7}\n",
            "{'loss': 0.3927, 'learning_rate': 2.1603733744957845e-05, 'epoch': 1.7}\n",
            "{'loss': 0.4067, 'learning_rate': 2.1482382179765355e-05, 'epoch': 1.71}\n",
            "{'loss': 0.4071, 'learning_rate': 2.136103061457287e-05, 'epoch': 1.72}\n",
            "{'loss': 0.371, 'learning_rate': 2.123967904938038e-05, 'epoch': 1.73}\n",
            "{'loss': 0.4006, 'learning_rate': 2.111832748418789e-05, 'epoch': 1.73}\n",
            "{'loss': 0.4026, 'learning_rate': 2.0996975918995403e-05, 'epoch': 1.74}\n",
            "{'loss': 0.3793, 'learning_rate': 2.0875624353802917e-05, 'epoch': 1.75}\n",
            "{'loss': 0.4079, 'learning_rate': 2.075427278861043e-05, 'epoch': 1.75}\n",
            "{'loss': 0.3753, 'learning_rate': 2.063292122341794e-05, 'epoch': 1.76}\n",
            "{'loss': 0.3851, 'learning_rate': 2.051156965822545e-05, 'epoch': 1.77}\n",
            "{'loss': 0.4009, 'learning_rate': 2.0390218093032965e-05, 'epoch': 1.78}\n",
            "{'loss': 0.3823, 'learning_rate': 2.0268866527840475e-05, 'epoch': 1.78}\n",
            "{'loss': 0.3854, 'learning_rate': 2.014751496264799e-05, 'epoch': 1.79}\n",
            "{'loss': 0.3955, 'learning_rate': 2.0026163397455503e-05, 'epoch': 1.8}\n",
            "{'loss': 0.393, 'learning_rate': 1.9904811832263013e-05, 'epoch': 1.81}\n",
            "{'loss': 0.391, 'learning_rate': 1.9783460267070523e-05, 'epoch': 1.81}\n",
            "{'loss': 0.3867, 'learning_rate': 1.9662108701878037e-05, 'epoch': 1.82}\n",
            "{'loss': 0.3874, 'learning_rate': 1.954075713668555e-05, 'epoch': 1.83}\n",
            "{'loss': 0.3792, 'learning_rate': 1.941940557149306e-05, 'epoch': 1.83}\n",
            "{'loss': 0.3972, 'learning_rate': 1.9298054006300575e-05, 'epoch': 1.84}\n",
            "{'loss': 0.3635, 'learning_rate': 1.9176702441108085e-05, 'epoch': 1.85}\n",
            "{'loss': 0.3766, 'learning_rate': 1.90553508759156e-05, 'epoch': 1.86}\n",
            "{'loss': 0.3958, 'learning_rate': 1.893399931072311e-05, 'epoch': 1.86}\n",
            "{'loss': 0.3908, 'learning_rate': 1.8812647745530623e-05, 'epoch': 1.87}\n",
            "{'loss': 0.3969, 'learning_rate': 1.8691296180338137e-05, 'epoch': 1.88}\n",
            "{'loss': 0.3758, 'learning_rate': 1.8569944615145647e-05, 'epoch': 1.89}\n",
            "{'loss': 0.3914, 'learning_rate': 1.8448593049953157e-05, 'epoch': 1.89}\n",
            "{'loss': 0.3861, 'learning_rate': 1.832724148476067e-05, 'epoch': 1.9}\n",
            "{'loss': 0.3804, 'learning_rate': 1.8205889919568185e-05, 'epoch': 1.91}\n",
            "{'loss': 0.3697, 'learning_rate': 1.8084538354375695e-05, 'epoch': 1.91}\n",
            "{'loss': 0.4014, 'learning_rate': 1.796318678918321e-05, 'epoch': 1.92}\n",
            "{'loss': 0.3817, 'learning_rate': 1.784183522399072e-05, 'epoch': 1.93}\n",
            "{'loss': 0.3901, 'learning_rate': 1.7720483658798233e-05, 'epoch': 1.94}\n",
            "{'loss': 0.3796, 'learning_rate': 1.7599132093605743e-05, 'epoch': 1.94}\n",
            "{'loss': 0.3936, 'learning_rate': 1.7477780528413257e-05, 'epoch': 1.95}\n",
            "{'loss': 0.391, 'learning_rate': 1.735642896322077e-05, 'epoch': 1.96}\n",
            "{'loss': 0.4012, 'learning_rate': 1.723507739802828e-05, 'epoch': 1.97}\n",
            "{'loss': 0.3714, 'learning_rate': 1.711372583283579e-05, 'epoch': 1.97}\n",
            "{'loss': 0.3818, 'learning_rate': 1.6992374267643305e-05, 'epoch': 1.98}\n",
            "{'loss': 0.3739, 'learning_rate': 1.687102270245082e-05, 'epoch': 1.99}\n",
            "{'loss': 0.3575, 'learning_rate': 1.674967113725833e-05, 'epoch': 2.0}\n",
            "{'loss': 0.3817, 'learning_rate': 1.6628319572065842e-05, 'epoch': 2.0}\n",
            "{'loss': 0.3382, 'learning_rate': 1.6506968006873353e-05, 'epoch': 2.01}\n",
            "{'loss': 0.3594, 'learning_rate': 1.6385616441680866e-05, 'epoch': 2.02}\n",
            "{'loss': 0.3682, 'learning_rate': 1.6264264876488377e-05, 'epoch': 2.02}\n",
            "{'loss': 0.3327, 'learning_rate': 1.614291331129589e-05, 'epoch': 2.03}\n",
            "{'loss': 0.3541, 'learning_rate': 1.6021561746103404e-05, 'epoch': 2.04}\n",
            "{'loss': 0.3483, 'learning_rate': 1.5900210180910914e-05, 'epoch': 2.05}\n",
            "{'loss': 0.3717, 'learning_rate': 1.5778858615718425e-05, 'epoch': 2.05}\n",
            "{'loss': 0.3494, 'learning_rate': 1.565750705052594e-05, 'epoch': 2.06}\n",
            "{'loss': 0.3479, 'learning_rate': 1.5536155485333452e-05, 'epoch': 2.07}\n",
            "{'loss': 0.3394, 'learning_rate': 1.5414803920140962e-05, 'epoch': 2.08}\n",
            "{'loss': 0.367, 'learning_rate': 1.5293452354948476e-05, 'epoch': 2.08}\n",
            "{'loss': 0.3678, 'learning_rate': 1.5172100789755986e-05, 'epoch': 2.09}\n",
            "{'loss': 0.3441, 'learning_rate': 1.5050749224563498e-05, 'epoch': 2.1}\n",
            "{'loss': 0.3629, 'learning_rate': 1.492939765937101e-05, 'epoch': 2.1}\n",
            "{'loss': 0.3487, 'learning_rate': 1.4808046094178524e-05, 'epoch': 2.11}\n",
            "{'loss': 0.3277, 'learning_rate': 1.4686694528986036e-05, 'epoch': 2.12}\n",
            "{'loss': 0.3681, 'learning_rate': 1.456534296379355e-05, 'epoch': 2.13}\n",
            "{'loss': 0.3392, 'learning_rate': 1.4443991398601058e-05, 'epoch': 2.13}\n",
            "{'loss': 0.3498, 'learning_rate': 1.4322639833408572e-05, 'epoch': 2.14}\n",
            "{'loss': 0.3548, 'learning_rate': 1.4201288268216084e-05, 'epoch': 2.15}\n",
            "{'loss': 0.3683, 'learning_rate': 1.4079936703023596e-05, 'epoch': 2.16}\n",
            "{'loss': 0.3478, 'learning_rate': 1.395858513783111e-05, 'epoch': 2.16}\n",
            "{'loss': 0.345, 'learning_rate': 1.383723357263862e-05, 'epoch': 2.17}\n",
            "{'loss': 0.3577, 'learning_rate': 1.3715882007446132e-05, 'epoch': 2.18}\n",
            "{'loss': 0.3589, 'learning_rate': 1.3594530442253644e-05, 'epoch': 2.18}\n",
            "{'loss': 0.3771, 'learning_rate': 1.3473178877061158e-05, 'epoch': 2.19}\n",
            "{'loss': 0.3463, 'learning_rate': 1.335182731186867e-05, 'epoch': 2.2}\n",
            "{'loss': 0.3679, 'learning_rate': 1.3230475746676182e-05, 'epoch': 2.21}\n",
            "{'loss': 0.3538, 'learning_rate': 1.3109124181483692e-05, 'epoch': 2.21}\n",
            "{'loss': 0.3574, 'learning_rate': 1.2987772616291204e-05, 'epoch': 2.22}\n",
            "{'loss': 0.3714, 'learning_rate': 1.2866421051098718e-05, 'epoch': 2.23}\n",
            "{'loss': 0.3643, 'learning_rate': 1.274506948590623e-05, 'epoch': 2.24}\n",
            "{'loss': 0.3342, 'learning_rate': 1.2623717920713744e-05, 'epoch': 2.24}\n",
            "{'loss': 0.3605, 'learning_rate': 1.2502366355521252e-05, 'epoch': 2.25}\n",
            "{'loss': 0.3537, 'learning_rate': 1.2381014790328768e-05, 'epoch': 2.26}\n",
            "{'loss': 0.3408, 'learning_rate': 1.2259663225136278e-05, 'epoch': 2.26}\n",
            "{'loss': 0.361, 'learning_rate': 1.213831165994379e-05, 'epoch': 2.27}\n",
            "{'loss': 0.3538, 'learning_rate': 1.2016960094751304e-05, 'epoch': 2.28}\n",
            "{'loss': 0.3439, 'learning_rate': 1.1895608529558814e-05, 'epoch': 2.29}\n",
            "{'loss': 0.3652, 'learning_rate': 1.1774256964366328e-05, 'epoch': 2.29}\n",
            "{'loss': 0.3605, 'learning_rate': 1.1652905399173838e-05, 'epoch': 2.3}\n",
            "{'loss': 0.3371, 'learning_rate': 1.1531553833981352e-05, 'epoch': 2.31}\n",
            "{'loss': 0.3613, 'learning_rate': 1.1410202268788864e-05, 'epoch': 2.32}\n",
            "{'loss': 0.3504, 'learning_rate': 1.1288850703596376e-05, 'epoch': 2.32}\n",
            "{'loss': 0.3417, 'learning_rate': 1.1167499138403888e-05, 'epoch': 2.33}\n",
            "{'loss': 0.3268, 'learning_rate': 1.10461475732114e-05, 'epoch': 2.34}\n",
            "{'loss': 0.3606, 'learning_rate': 1.0924796008018912e-05, 'epoch': 2.34}\n",
            "{'loss': 0.3688, 'learning_rate': 1.0803444442826424e-05, 'epoch': 2.35}\n",
            "{'loss': 0.3588, 'learning_rate': 1.0682092877633937e-05, 'epoch': 2.36}\n",
            "{'loss': 0.3422, 'learning_rate': 1.0560741312441448e-05, 'epoch': 2.37}\n",
            "{'loss': 0.3716, 'learning_rate': 1.0439389747248961e-05, 'epoch': 2.37}\n",
            "{'loss': 0.3613, 'learning_rate': 1.0318038182056472e-05, 'epoch': 2.38}\n",
            "{'loss': 0.3579, 'learning_rate': 1.0196686616863985e-05, 'epoch': 2.39}\n",
            "{'loss': 0.3542, 'learning_rate': 1.0075335051671497e-05, 'epoch': 2.4}\n",
            "{'loss': 0.3443, 'learning_rate': 9.95398348647901e-06, 'epoch': 2.4}\n",
            "{'loss': 0.343, 'learning_rate': 9.832631921286521e-06, 'epoch': 2.41}\n",
            "{'loss': 0.3593, 'learning_rate': 9.711280356094033e-06, 'epoch': 2.42}\n",
            "{'loss': 0.3644, 'learning_rate': 9.589928790901545e-06, 'epoch': 2.42}\n",
            " 81% 166500/206013 [2:05:46<27:28, 23.97it/s]Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 619, in save\n",
            "    _save(obj, opened_zipfile, pickle_module, pickle_protocol, _disable_byteorder_record)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 853, in _save\n",
            "    zip_file.write_record(name, storage.data_ptr(), num_bytes)\n",
            "RuntimeError: [enforce fail at inline_container.cc:588] . PytorchStreamWriter failed writing file data/2: file write failed\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/run.py\", line 208, in <module>\n",
            "    main()\n",
            "  File \"/content/run.py\", line 163, in main\n",
            "    trainer.train()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1555, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1922, in _inner_training_loop\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2282, in _maybe_log_save_evaluate\n",
            "    self._save_checkpoint(model, trial, metrics=metrics)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2387, in _save_checkpoint\n",
            "    torch.save(self.optimizer.state_dict(), os.path.join(output_dir, OPTIMIZER_NAME))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 618, in save\n",
            "    with _open_zipfile_writer(f) as opened_zipfile:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 466, in __exit__\n",
            "    self.file_like.write_end_of_file()\n",
            "RuntimeError: [enforce fail at inline_container.cc:424] . unexpected pos 52132032 vs 52131920\n",
            " 81% 166500/206013 [2:05:46<29:50, 22.06it/s]\n"
          ]
        }
      ],
      "source": [
        "!python3 run.py --do_train --task nli --dataset snli --output_dir ./trained_model/"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP3sij6HIjRwRgC6nEia4jm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}